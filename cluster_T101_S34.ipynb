{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import myResnet\n",
    "import os\n",
    "import math\n",
    "# from AE import AE_conv\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../saved_model/cifar100_resnet101_Fitnet_32*32.pkl\n",
      "saved_model/S18_T101_cifar100.pkl\n",
      "saved_curve/S18_T101_cifar100_curve.jpg\n"
     ]
    }
   ],
   "source": [
    "###hyper-parameters settings\n",
    "\n",
    "batch_size = 128\n",
    "lr = 0.1\n",
    "num_epochs = 200\n",
    "\n",
    "dataset = 'cifar100'\n",
    "t_name = 'resnet101'\n",
    "s_name = 'resnet18'\n",
    "\n",
    "t_path = '../saved_model/' + dataset + '_' + t_name + '_Fitnet_32*32.pkl'\n",
    "s_path = 'saved_model/S' + s_name[-2:] + '_T' + t_name[-3:] + '_' + dataset + '.pkl'\n",
    "curve_path = 'saved_curve/S' + s_name[-2:] + '_T' + t_name[-3:] + '_' + dataset + '_curve.jpg'\n",
    "AE4_path = 'AE_conv_' + t_name + '_' + dataset + 'features4.pkl'\n",
    "AE3_path = 'AE_conv_' + t_name + '_' + dataset + 'features3.pkl'\n",
    "\n",
    "print(t_path)\n",
    "print(s_path)\n",
    "print(curve_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_process = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
    "                              transforms.RandomHorizontalFlip(),\n",
    "                              transforms.ToTensor(), \n",
    "                              transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n",
    "test_process = transforms.Compose([transforms.ToTensor(),\n",
    "                                   transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n",
    "if dataset == 'cifar100':\n",
    "    train_data = datasets.CIFAR100(root='/root/userfolder/xxx/cifar100', transform=train_process, train=True, download=True)\n",
    "    test_data = datasets.CIFAR100(root='/root/userfolder/xxx/cifar100', transform=test_process, train=False, download=True)\n",
    "    num_classes = 100\n",
    "elif dataset == 'cifar10':\n",
    "    train_data = datasets.CIFAR10(root='/root/userfolder/xxx/cifar10', transform=train_process, train=True, download=True)\n",
    "    test_data = datasets.CIFAR10(root='/root/userfolder/xxx/cifar10', transform=test_process, train=False, download=True)\n",
    "    num_classes = 10\n",
    "train_dataLoader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataLoader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#params of Student: 1329760\n"
     ]
    }
   ],
   "source": [
    "Teacher = torch.load(t_path)\n",
    "#Student = torchvision.models.resnet34()\n",
    "if(s_name == 'resnet18'):\n",
    "    Student = myResnet.resnet18(num_classes=num_classes)\n",
    "    Student.fc = nn.Linear(128, num_classes)\n",
    "    Student.branch1 = myResnet.conv1x1(16, 128)\n",
    "    Student.branch2 = myResnet.conv1x1(32, 256)\n",
    "    Student.branch3 = myResnet.conv1x1(64, 512)\n",
    "    Student.branch4 = myResnet.conv1x1(128, 1024)\n",
    "elif(s_name == 'resnet34'):\n",
    "    Student = myResnet.resnet34(num_classes=num_classes)\n",
    "    Student.fc = nn.Linear(128, num_classes)\n",
    "    Student.branch1 = myResnet.conv1x1(16, 128)\n",
    "    Student.branch2 = myResnet.conv1x1(32, 256)\n",
    "    Student.branch3 = myResnet.conv1x1(64, 512)\n",
    "    Student.branch4 = myResnet.conv1x1(128, 1024)\n",
    "elif(s_name == 'resnet50'):\n",
    "    Student = myResnet.resnet50(num_classes=num_classes)\n",
    "    Student.fc = nn.Linear(512, num_classes)\n",
    "    Student.branch1 = myResnet.conv1x1(64, 128)\n",
    "    Student.branch2 = myResnet.conv1x1(128, 256)\n",
    "    Student.branch3 = myResnet.conv1x1(256, 512)\n",
    "    Student.branch4 = myResnet.conv1x1(512, 1024)\n",
    "\n",
    "# AE4 = AE_conv(1024, 4)\n",
    "# AE4.load_state_dict(torch.load(AE4_path))\n",
    "# AE3 = AE_conv(512, 2)\n",
    "# AE3.load_state_dict(torch.load(AE3_path))\n",
    "print('#params of Student:', sum([para.numel() for para in Student.parameters()]))\n",
    "for para in Teacher.parameters():\n",
    "    para.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###transfer to GPU\n",
    "Teacher = Teacher.cuda()\n",
    "Student = Student.cuda()\n",
    "# AE4 = AE4.cuda()\n",
    "# AE3 = AE3.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    model.eval()\n",
    "    test_acc = 0\n",
    "    for img, label in test_dataLoader:\n",
    "        img = img.cuda()\n",
    "        label = label.cuda()\n",
    "        out = model(img)\n",
    "        out = out[0]\n",
    "        _, pred = torch.max(out, 1)\n",
    "        num_correct = (pred == label).sum()\n",
    "        test_acc += num_correct.data.item()\n",
    "    return test_acc/len(test_data)\n",
    "\n",
    "def correlation(feature):\n",
    "    CR = []\n",
    "    for i in range(feature.size(0)):\n",
    "        tmp = feature[i].reshape(1,feature[i].size(0), feature[i].size(1), feature[i].size(2))\n",
    "        tmp = tmp.expand(feature.size(0), -1, -1, -1)\n",
    "#         print((tmp**2).sum(dim=[1,2,3]).sqrt())\n",
    "        cos_sim = (tmp*feature).sum(dim=[1,2,3]) / (tmp.detach()**2).sum(dim=[1,2,3]).sqrt() / (feature.detach()**2).sum(dim=[1,2,3]).sqrt()\n",
    "        CR.append(cos_sim)\n",
    "    CR = torch.stack(CR)\n",
    "    return CR\n",
    "\n",
    "def cluster(feature, label, num_classes):\n",
    "    length = feature.size(0)\n",
    "    center_list = []\n",
    "    res = []\n",
    "    valid_center_list = []\n",
    "    for i in range(num_classes):\n",
    "        center_list.append([])\n",
    "    for i in range(length):\n",
    "        center_list[label[i]].append(feature[i])\n",
    "    for i in range(num_classes):\n",
    "        if(len(center_list[i]) > 0):\n",
    "            center_list[i] = sum(center_list[i]) / len(center_list[i])\n",
    "            valid_center_list.append(center_list[i])\n",
    "#     print(center_list[label[0]].size())\n",
    "    for i in range(length):\n",
    "        res.append(center_list[label[i]])\n",
    "    \n",
    "    return torch.stack(res), torch.stack(valid_center_list)\n",
    "\n",
    "def cluster_filter(feature, pred, label, num_classes, mode='intra'):  \n",
    "    length = feature.size(0)\n",
    "    center_list = []\n",
    "    res = []\n",
    "    valid_center_list = []\n",
    "    for i in range(num_classes):\n",
    "        center_list.append([])\n",
    "    for i in range(length):\n",
    "        if(pred[i] == label[i]):\n",
    "            center_list[label[i]].append(feature[i])\n",
    "    for i in range(num_classes):\n",
    "        if(len(center_list[i]) > 0):\n",
    "            center_list[i] = sum(center_list[i]) / len(center_list[i])\n",
    "            valid_center_list.append(center_list[i])\n",
    "#     print(center_list[label[0]].size())\n",
    "    for i in range(length):\n",
    "        if(len(center_list[label[i]]) > 0):\n",
    "            res.append(center_list[label[i]])\n",
    "        else:\n",
    "            res.append(feature[i])\n",
    "#     print(len(res), len(valid_center_list))\n",
    "    if mode == 'intra':\n",
    "        return torch.stack(res)#, torch.stack(valid_center_list)\n",
    "    elif(len(valid_center_list) > 0):\n",
    "        return torch.stack(valid_center_list)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def boundary_loss(T_mean, S_feature, label, criterion):\n",
    "    res = []\n",
    "    balance = 0.8\n",
    "    for i in range(S_feature.size(0)):\n",
    "        f = S_feature[i].reshape(1, S_feature[i].size(0), S_feature[i].size(1), S_feature[i].size(2))\n",
    "        f.expand(S_feature.size(0), -1, -1, -1)\n",
    "        diff = (T_mean - f).abs().sum(dim=[1,2,3])\n",
    "        _, idx = torch.min(diff, -1)\n",
    "        if label[idx] != label[i]:\n",
    "            res.append(balance*((T_mean[i].detach()-S_feature[i])**2).sum()/T_mean.size(1)/T_mean.size(2)/T_mean.size(3))\n",
    "        else:\n",
    "            res.append((1-balance)*((T_mean[i].detach()-S_feature[i])**2).sum()/T_mean.size(1)/T_mean.size(2)/T_mean.size(3))\n",
    "    if(len(res) == 0):\n",
    "        return 0.0\n",
    "    return torch.stack(res).sum()/len(res)\n",
    "\n",
    "\n",
    "def correlation_logits(feature):\n",
    "    CR = []\n",
    "    for i in range(feature.size(0)):\n",
    "        tmp = feature[i].reshape(1,feature[i].size(0))\n",
    "        tmp = tmp.expand(feature.size(0), -1)   \n",
    "        cos_sim = (tmp*feature).sum(dim=-1) / (tmp.detach()**2).sum(dim=-1).sqrt() / (feature.detach()**2).sum(dim=-1).sqrt()\n",
    "        CR.append(cos_sim)\n",
    "    CR = torch.stack(CR)\n",
    "    return CR\n",
    "\n",
    "def reg_loss(feature, label, num_classes):\n",
    "    center, _ = cluster(feature, label, num_classes)\n",
    "    intra_var_loss = MSE(feature, center.detach())\n",
    "    \n",
    "    t_center = center.sum(dim=0)\n",
    "    t_center = t_center.reshape(1, center.size(1))\n",
    "    \n",
    "    t_center = t_center.expand(center.size(0), -1)\n",
    "    inter_var_loss = MSE(center, t_center.detach())\n",
    "    \n",
    "    return intra_var_loss, inter_var_loss\n",
    "\n",
    "def cosine_sim(x, y):\n",
    "    dimention = [i for i in range(1,len(x.size()))]\n",
    "    return (x*y).sum(dim=dimention) / (x**2).sum(dim=dimention).sqrt() / (y**2).sum(dim=dimention).sqrt()\n",
    "\n",
    "def margin_loss(t_feature, t_prob, s_feature, s_prob, label, num_classes, criterion):\n",
    "    t_category_list = []\n",
    "    s_category_list = []\n",
    "    t_category_feature = []\n",
    "    s_category_feature = []\n",
    "    t_category_prob = []\n",
    "    _, t_pred = torch.max(t_prob, 1)\n",
    "    _, s_pred = torch.max(s_prob, 1)\n",
    "    for i in range(num_classes):\n",
    "        t_category_list.append([])\n",
    "        s_category_list.append([])\n",
    "        t_category_feature.append([])\n",
    "        s_category_feature.append([])\n",
    "        t_category_prob.append([])\n",
    "        \n",
    "    for i in range(len(t_pred)):\n",
    "        if(t_pred[i] == label[i]):\n",
    "            t_category_list[label[i]].append(t_feature[i])\n",
    "            s_category_list[label[i]].append(s_feature[i])\n",
    "            \n",
    "            t_category_prob[label[i]].append(t_prob[i])\n",
    "            t_category_feature[label[i]].append(t_feature[i])\n",
    "            s_category_feature[label[i]].append(s_feature[i])\n",
    "    \n",
    "    category_list = []\n",
    "    for i in range(num_classes):\n",
    "        if(len(t_category_list[i]) > 0):\n",
    "            t_category_list[i] = sum(t_category_list[i]) / len(t_category_list[i])\n",
    "            s_category_list[i] = sum(s_category_list[i]) / len(s_category_list[i])\n",
    "            category_list.append(i)\n",
    "    \n",
    "    t_cor = []\n",
    "    s_cor = []\n",
    "    for (idx1,i) in enumerate(category_list):\n",
    "        t_category_feature[i] = torch.stack(t_category_feature[i])\n",
    "        t_category_prob[i] = torch.stack(t_category_prob[i])\n",
    "        s_category_feature[i] = torch.stack(s_category_feature[i])\n",
    "        \n",
    "        for idx2 in range(idx1+1, len(category_list)):\n",
    "            j = category_list[idx2]\n",
    "            _, sample_id = torch.max(t_category_prob[i][:,j], 0)\n",
    "            t_class_cor = (t_category_feature[i][sample_id] - t_category_list[j]).detach()\n",
    "            s_class_cor = s_category_feature[i][sample_id] - s_category_list[j].detach()\n",
    "            t_cor.append(t_class_cor)\n",
    "            s_cor.append(s_class_cor)\n",
    "#             t_cor = t_class_cor\n",
    "#             s_cor = s_class_cor\n",
    "    return criterion(torch.stack(t_cor),torch.stack(s_cor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-570742542a16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mt_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTeacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0ms_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msf2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msf3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msf4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStudent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m#         print(sf1.size(), sf2.size(), sf3.size(), sf4.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 5)"
     ]
    }
   ],
   "source": [
    "CE = nn.CrossEntropyLoss()\n",
    "MSE = nn.MSELoss()\n",
    "\n",
    "CE = CE.cuda()\n",
    "MSE = MSE.cuda()\n",
    "\n",
    "loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "#initialize center list\n",
    "center_list = []\n",
    "for i in range(num_classes):\n",
    "    center_list.append([])\n",
    "\n",
    "use_filter = False\n",
    "    \n",
    "for epoch in range(num_epochs):\n",
    "    Student.train()\n",
    "    if epoch < 80:\n",
    "        optimizer = optim.SGD(Student.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0001)\n",
    "    elif epoch < 120:\n",
    "        optimizer = optim.SGD(Student.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)\n",
    "    else:\n",
    "        optimizer = optim.SGD(Student.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0001)\n",
    "       \n",
    "    cost = 0.0\n",
    "    cnt = 0\n",
    "    num_correct = 0\n",
    "    s_tick = time.time()\n",
    "\n",
    "    for i, (img, label) in enumerate(train_dataLoader):\n",
    "        s = time.time()\n",
    "        img, label = img.cuda(), label.cuda()\n",
    "        t_out, tf1, tf2, tf3, tf4 = Teacher(img)\n",
    "        s_out, sf1, sf2, sf3, sf4 = Student(img)\n",
    "\n",
    "#         print(sf1.size(), sf2.size(), sf3.size(), sf4.size())\n",
    "        _, t_pred = torch.max(t_out, 1)\n",
    "        _, s_pred = torch.max(s_out, 1)\n",
    "        if use_filter:\n",
    "#             TF4, T_center_list = cluster_filter(tf4, t_pred, label, num_classes)\n",
    "#             SF4, S_center_list = cluster_filter(sf4, s_pred, label, num_classes)\n",
    "#             T_OUT, T_logits = cluster_filter(t_out, t_pred, label, num_classes)\n",
    "#             S_OUT, S_logits = cluster_filter(s_out, s_pred, label, num_classes)\n",
    "            \n",
    "            \n",
    "            TF4 = cluster_filter(tf4, t_pred, label, num_classes)\n",
    "            SF4 = cluster_filter(sf4, s_pred, label, num_classes)\n",
    "            T_OUT = cluster_filter(t_out, t_pred, label, num_classes)\n",
    "            S_OUT = cluster_filter(s_out, s_pred, label, num_classes)\n",
    "            \n",
    "            T_center_list = cluster_filter(tf4, s_pred, label, num_classes, mode='inter')\n",
    "            S_center_list = cluster_filter(sf4, s_pred, label, num_classes, mode='inter')\n",
    "            \n",
    "            T_logits = cluster_filter(t_out, s_pred, label, num_classes, mode='inter')\n",
    "            S_logits = cluster_filter(s_out, s_pred, label, num_classes, mode='inter')\n",
    "        else:\n",
    "            TF4, T_center_list = cluster(tf4, label, num_classes)\n",
    "            SF4, S_center_list = cluster(sf4, label, num_classes)\n",
    "            T_OUT, T_logits = cluster(t_out, label, num_classes)\n",
    "            S_OUT, S_logits = cluster(s_out, label, num_classes)\n",
    "#         class_correlation = torch.tensor(0.0, requires_grad=False).cuda()\n",
    "#         if(len(T_center_list) > 0):\n",
    "#             T_cor = correlation(T_center_list)\n",
    "#             S_cor = correlation(S_center_list)\n",
    "#             class_correlation += MSE(T_cor.detach(), S_cor) / T_cor.size(1)\n",
    "#         if(len(T_logits) > 0):\n",
    "#             T_cor_logits = correlation_logits(T_logits)\n",
    "#             S_cor_logits = correlation_logits(S_logits)\n",
    "#             class_correlation += MSE(T_cor_logits.detach(), S_cor_logits) / T_cor_logits.size(1)\n",
    "        \n",
    "#         sim = MSE(cosine_sim(sf4, SF4), cosine_sim(tf4, TF4).detach()) + \\\n",
    "#                 MSE(cosine_sim(s_out, S_OUT), cosine_sim(t_out, T_OUT).detach())\n",
    "        \n",
    "        sim = MSE((tf4-TF4).detach(), sf4-SF4.detach()) / TF4.size(1) / TF4.size(2) / TF4.size(3) + \\\n",
    "                MSE((t_out-T_OUT).detach(), s_out-S_OUT.detach()) / T_OUT.size(1)\n",
    "#         class_correlation = margin_loss(tf4, t_out, sf4, s_out, label, num_classes, MSE) / tf4.size(1) / tf4.size(2) / tf4.size(3)\n",
    "        class_correlation = margin_loss(t_out, t_out, s_out, s_out, label, num_classes, MSE) / t_out.size(1)\n",
    "        cluster_loss =  0.002*sim\n",
    "#         TF3 = cluster(tf3, label, num_classes)\n",
    "#         SF4 = cluster(sf4, label, num_classes)  \n",
    "#         _, t_b4 = AE4(tf4)\n",
    "#         _, s_b4 = AE4(sf4)\n",
    "        \n",
    "#         _, t_b3 = AE3(tf3)\n",
    "#         _, s_b3 = AE3(sf3)\n",
    "        temperature = 4\n",
    "        cross_entropy = CE(s_out, label)# + 0.4*CE(s_out1, label) + 0.6*CE(s_out2, label) + 0.8*CE(s_out3, label)\n",
    "        loss_KD = 0.00001*MSE(t_out.detach(), s_out)\n",
    "#         loss_KD = - (F.softmax(t_out / temperature, 1).detach() *\n",
    "#                      (F.log_softmax(s_out / temperature, 1) - F.log_softmax(t_out / temperature, 1).detach())).sum() / batch_size\n",
    "#         loss_KD1 = - (F.softmax(t_out / temperature, 1).detach() *\n",
    "#                      (F.log_softmax(s_out1 / temperature, 1) - F.log_softmax(t_out / temperature, 1).detach())).sum() / batch_size\n",
    "#         loss_KD2 = - (F.softmax(t_out / temperature, 1).detach() *\n",
    "#                      (F.log_softmax(s_out2 / temperature, 1) - F.log_softmax(t_out / temperature, 1).detach())).sum() / batch_size\n",
    "#         loss_KD3 = - (F.softmax(t_out / temperature, 1).detach() *\n",
    "#                      (F.log_softmax(s_out3 / temperature, 1) - F.log_softmax(t_out / temperature, 1).detach())).sum() / batch_size\n",
    "        \n",
    "#         sim = MSE((tf4-TF4).detach(), sf4-SF4) / TF4.size(1) / TF4.size(2) / TF4.size(3)\n",
    "#         if(epoch > 20):\n",
    "#         cr4 = correlation(sf4.detach())\n",
    "#         cr3 = correlation(sf3)\n",
    "#         cr2 = correlation(sf2)\n",
    "#         cr1 = correlation(sf1)\n",
    "#         self_sup_Loss = MSE(cr4, cr3)\n",
    "#         loss = cross_entropy + loss_KD1 + loss_KD2 + loss_KD3\n",
    "#         loss = cross_entropy + MSE(F.softmax(t_out / temperature, 1).detach(), F.softmax(s_out /temperature, 1))\n",
    "#         soft_supervision = MSE(t_out.detach(), s_out) / t_out.size(1)\n",
    "        alpha = 1\n",
    "        intra_var_loss, inter_var_loss = reg_loss(s_out, label, num_classes)\n",
    "        loss = alpha*cross_entropy +   cluster_loss + 0.1*intra_var_loss #+ (1-alpha)*loss_KD# + 0.1 * intra_var_loss / inter_var_loss\n",
    "#         else:\n",
    "#             loss= loss_KD + 0.00001*sim\n",
    "#         print(alpha*cross_entropy, sim, (1-alpha)*loss_KD)\n",
    "        _, pred = torch.max(s_out, 1)\n",
    "    \n",
    "    \n",
    "        num_correct += (pred == label).sum().item()\n",
    "        cost += loss.item()\n",
    "        cnt += 1\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        e = time.time()\n",
    "        if(i%5 == 0):\n",
    "            print('epoch %d: %d/%d, time:%.2fs  ce:%.3f sim:%.3f cor:%.3f cluster:%.3f kd:%.3f'%(epoch, cnt, math.ceil(len(train_data)/batch_size), e-s, cross_entropy.item(), sim.item(),class_correlation.item(), cluster_loss.item(),loss_KD.item()), end='\\r')\n",
    "    e_tick = time.time()\n",
    "    loss_list.append(cost/cnt)\n",
    "    train_acc_list.append(num_correct/len(train_data))\n",
    "    test_acc_list.append(test(Student))\n",
    "    log = 'epoch:%d loss:%f train_acc:%f test_acc:%f time:%.2fs'%(epoch, cost/cnt, num_correct/len(train_data), test(Student), e_tick-s_tick)\n",
    "    print(log)\n",
    "    open(log_path, 'a').write(log)\n",
    "    torch.save(Student.state_dict(), s_path)\n",
    "print('Maximum test accuracy:', max(test_acc_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交叉熵+AE压缩最后一层：91.22\n",
    "交叉熵：90.94\n",
    "交叉熵+AE压缩最后二层：90.45\n",
    "交叉熵+最后两层特征图逼近：90.75\n",
    "\n",
    "四个交叉熵：91.64\n",
    "四个交叉熵+3个浅层软标签约束：90.93\n",
    "四个交叉熵+四个KL软标签约束：91.36\n",
    "最后一层KL软约束：92.01\n",
    "四个KL软约束：91.35\n",
    "交叉熵+3个浅层软约束：90.81\n",
    "最后一层softmax后的MSE软约束：90\n",
    "交叉熵+最后一层KL软约束：91.95\n",
    "交叉熵+logits的MSE：91.74\n",
    "四个交叉熵+四个logits的MSE软标签约束：91.75\n",
    "四个交叉熵+四个logtis的MSE软标签约束+最后一层AE的约束:91.9\n",
    "交叉熵+四层特征图逼近：90.97\n",
    "交叉熵+四层特征图逼近+AE压缩最后一层：91.63\n",
    "交叉熵+四层特征图逼近（大权重）+AE压缩最后一层：91.18\n",
    "四个交叉熵+四个logits的MSE+最后一层AE：91.75\n",
    "交叉熵+cluster：91.37\n",
    "交叉熵+Fitnet最后一层：91.33\n",
    "交叉熵+cluster+logits的MSE：91.43\n",
    "交叉熵+Fitnet最后一层+logits的MSE：91.56\n",
    "交叉熵+KL散度+cluster：91.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x_axis = range(num_epochs)\n",
    "plt.figure(figsize=(10,13))\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(x_axis, loss_list, 'r-')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(x_axis, train_acc_list, 'g-')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('train accuracy')\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(x_axis, test_acc_list, 'b-')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('test accuracy')\n",
    "\n",
    "plt.savefig(curve_path, dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('intra_loss1.npy', intra_CS_loss_list)\n",
    "np.save('inter_loss1.npy', inter_CS_loss_list)\n",
    "np.save('loss1.npy', loss_list)\n",
    "np.save('train_acc1.npy', train_acc_list)\n",
    "np.save('test_acc1.npy', test_acc_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
